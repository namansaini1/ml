title,abstract,authors,publication_date
"Understanding Deep Learning Requires Rethinking Generalization","Deep learning has achieved remarkable success in various domains, yet its generalization capabilities remain poorly understood. In this paper, we investigate the generalization properties of deep neural networks and propose a new perspective on the role of overparameterization.","Nicolas Papernot, et al.","2020-05-01"
"Attention Is All You Need","We propose a new architecture for machine translation that relies entirely on an attention mechanism, dispensing with recurrence and convolutions entirely. Our model achieves state-of-the-art results on several benchmarks.","Ashish Vaswani, et al.","2017-06-12"
"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","We introduce BERT, a new method for pre-training language representations which obtains state-of-the-art results on a wide array of natural language processing tasks.","Jacob Devlin, et al.","2018-10-11"
"Generative Adversarial Nets","We propose a new framework for estimating generative models via an adversarial process. We show that this framework can generate realistic samples from a variety of distributions.","Ian Goodfellow, et al.","2014-06-10"
"ImageNet Classification with Deep Convolutional Neural Networks","We present a deep convolutional neural network that achieved state-of-the-art results on the ImageNet classification benchmark. Our model is trained on a large dataset and demonstrates the effectiveness of deep learning.","Alex Krizhevsky, et al.","2012-09-30"